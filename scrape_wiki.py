import requests
from bs4 import BeautifulSoup
import time
from tqdm import tqdm
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. í•µì‹¬ ë¬¸ì„œ URL ë¦¬ìŠ¤íŠ¸ (52ê°œ) ---
# íƒìƒ‰ ê³¼ì • ì—†ì´ ë°”ë¡œ ì ‘ì†í•  'ì•Œì§œë°°ê¸°' ì£¼ì†Œë“¤ì…ë‹ˆë‹¤.
target_urls = [
    # ì£¼ì¸ê³µ ë° í•µì‹¬ ì¸ë¬¼
    "https://cyberpunk.fandom.com/wiki/V_(character)",
    "https://cyberpunk.fandom.com/wiki/Johnny_Silverhand",
    "https://cyberpunk.fandom.com/wiki/Jackie_Welles",
    "https://cyberpunk.fandom.com/wiki/Judy_Alvarez",
    "https://cyberpunk.fandom.com/wiki/Panam_Palmer",
    "https://cyberpunk.fandom.com/wiki/Goro_Takemura",
    "https://cyberpunk.fandom.com/wiki/Adam_Smasher",
    "https://cyberpunk.fandom.com/wiki/Alt_Cunningham",
    "https://cyberpunk.fandom.com/wiki/Rogue_Amendiares",
    "https://cyberpunk.fandom.com/wiki/Kerry_Eurodyne",
    "https://cyberpunk.fandom.com/wiki/River_Ward",
    "https://cyberpunk.fandom.com/wiki/Evelyn_Parker",
    "https://cyberpunk.fandom.com/wiki/Dexter_DeShawn",
    "https://cyberpunk.fandom.com/wiki/Solomon_Reed",
    "https://cyberpunk.fandom.com/wiki/Song_So_Mi",
    "https://cyberpunk.fandom.com/wiki/Rosalind_Myers",
    "https://cyberpunk.fandom.com/wiki/Kurt_Hansen",
    
    # ì•„ë¼ì‚¬ì¹´ ê°€ë¬¸
    "https://cyberpunk.fandom.com/wiki/Saburo_Arasaka",
    "https://cyberpunk.fandom.com/wiki/Yorinobu_Arasaka",
    "https://cyberpunk.fandom.com/wiki/Hanako_Arasaka",

    # ì£¼ìš” ê¸°ì—… (Corporations)
    "https://cyberpunk.fandom.com/wiki/Arasaka",
    "https://cyberpunk.fandom.com/wiki/Militech",
    "https://cyberpunk.fandom.com/wiki/Kang_Tao",
    "https://cyberpunk.fandom.com/wiki/Biotechnica",
    "https://cyberpunk.fandom.com/wiki/Trauma_Team_International",
    "https://cyberpunk.fandom.com/wiki/Zetatech",
    "https://cyberpunk.fandom.com/wiki/Night_Corp",

    # ê°±ë‹¨ ë° ì„¸ë ¥ (Gangs & Factions)
    "https://cyberpunk.fandom.com/wiki/Maelstrom",
    "https://cyberpunk.fandom.com/wiki/Valentinos",
    "https://cyberpunk.fandom.com/wiki/Voodoo_Boys",
    "https://cyberpunk.fandom.com/wiki/Animals_(Gang)",
    "https://cyberpunk.fandom.com/wiki/Tyger_Claws",
    "https://cyberpunk.fandom.com/wiki/6th_Street",
    "https://cyberpunk.fandom.com/wiki/The_Mox",
    "https://cyberpunk.fandom.com/wiki/Scavengers",
    "https://cyberpunk.fandom.com/wiki/Wraiths",
    "https://cyberpunk.fandom.com/wiki/Aldecaldos",
    "https://cyberpunk.fandom.com/wiki/Barghest",

    # ì£¼ìš” ì§€ì—­ (Locations)
    "https://cyberpunk.fandom.com/wiki/Night_City",
    "https://cyberpunk.fandom.com/wiki/Watson",
    "https://cyberpunk.fandom.com/wiki/Westbrook",
    "https://cyberpunk.fandom.com/wiki/City_Center",
    "https://cyberpunk.fandom.com/wiki/Heywood",
    "https://cyberpunk.fandom.com/wiki/Santo_Domingo",
    "https://cyberpunk.fandom.com/wiki/Pacifica",
    "https://cyberpunk.fandom.com/wiki/Dogtown",
    "https://cyberpunk.fandom.com/wiki/Afterlife",
    "https://cyberpunk.fandom.com/wiki/Konpeki_Plaza",

    # í•µì‹¬ ì„¤ì • (Lore & Tech)
    "https://cyberpunk.fandom.com/wiki/Cyberware",
    "https://cyberpunk.fandom.com/wiki/Cyberpsychosis",
    "https://cyberpunk.fandom.com/wiki/Netrunner",
    "https://cyberpunk.fandom.com/wiki/Braindance",
    "https://cyberpunk.fandom.com/wiki/Blackwall",
    "https://cyberpunk.fandom.com/wiki/Relic"
]

output_file = "cyberpunk_lore.txt"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

print(f"ğŸš€ ì´ {len(target_urls)}ê°œì˜ í•µì‹¬ ë¬¸ì„œ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

with open(output_file, "w", encoding="utf-8") as f:
    # ì‹¤íŒ¨í•œ URLì„ ê¸°ë¡í•  ë¦¬ìŠ¤íŠ¸
    failed_urls = []
    
    for i, url in enumerate(tqdm(target_urls, desc="ì§„í–‰ ì¤‘")):
        try:
            # verify=Falseë¡œ SSL ì—ëŸ¬ ìš°íšŒ
            response = requests.get(url, headers=headers, verify=False, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")
                
                # ì œëª© ì¶”ì¶œ
                title_tag = soup.find("h1", {"id": "firstHeading"})
                title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                
                # ë³¸ë¬¸ ì¶”ì¶œ (ì¡ë‹¤í•œ ìš”ì†Œ ì œê±°)
                content_div = soup.find("div", {"class": "mw-parser-output"})
                if content_div:
                    for garbage in content_div.find_all(["div", "aside", "table", "figure"], class_=["toc", "infobox", "rail-module", "thumb"]):
                        garbage.decompose()
                        
                    paragraphs = content_div.find_all("p", recursive=False)
                    full_text = ""
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text:
                            full_text += text + "\n"
                    
                    if full_text:
                        f.write(f"[ë¬¸ì„œ ì œëª©: {title}]\n")
                        f.write(f"ì¶œì²˜: {url}\n")
                        f.write(full_text)
                        f.write("\n----\n\n")
            else:
                failed_urls.append(url)
                
            time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€

        except Exception as e:
            print(f"\nâš ï¸ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
            failed_urls.append(url)

print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! '{output_file}' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
if failed_urls:
    print(f"âš ï¸ {len(failed_urls)}ê°œì˜ ë¬¸ì„œëŠ” ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")