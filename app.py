import streamlit as st
import os
import sys
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•¨) ---
st.set_page_config(
    page_title="Cyberpunk 2077 Wiki AI",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- 2. í™˜ê²½ ë³€ìˆ˜ ë° ì´ˆê¸° ì„¤ì • ---
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
RAG_MODEL = "gpt-4o-mini"
DATA_FILE = "cyberpunk_shards.txt"
CHROMA_DIR = "./cyberpunk_chroma_db"

# --- 3. ì»¤ìŠ¤í…€ CSS (ì‚¬ì´ë²„í‘í¬ í…Œë§ˆ ë””ìì¸) ---
st.markdown("""
<style>
    /* ì „ì²´ ë°°ê²½ ë° í°íŠ¸ */
    @import url('https://fonts.googleapis.com/css2?family=Rajdhani:wght@500;700&display=swap');
    
    .stApp {
        background-color: #050505;
        font-family: 'Rajdhani', sans-serif;
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    h1 {
        color: #FCEE0A !important;
        text-transform: uppercase;
        text-shadow: 2px 2px 0px #00F0FF;
        font-weight: 800 !important;
        letter-spacing: 2px;
    }
    
    /* ìº¡ì…˜ ìŠ¤íƒ€ì¼ */
    .stCaption {
        color: #00F0FF !important;
        font-size: 1.1em !important;
        border-left: 3px solid #FCEE0A;
        padding-left: 10px;
    }

    /* ì±„íŒ… ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ */
    .stChatMessage {
        background-color: #1a1a1a;
        border: 1px solid #333;
        border-radius: 0px !important; /* ê°ì§„ í…Œë‘ë¦¬ */
        margin-bottom: 10px;
    }

    /* ìœ ì € ë©”ì‹œì§€ (ì˜¤ë¥¸ìª½ ì •ë ¬ ëŠë‚Œ) */
    div[data-testid="stChatMessage"]:nth-child(odd) {
        border-left: 5px solid #FCEE0A;
    }

    /* AI ë©”ì‹œì§€ (ì™¼ìª½ ì •ë ¬ ëŠë‚Œ) */
    div[data-testid="stChatMessage"]:nth-child(even) {
        border-right: 5px solid #00F0FF;
        background-color: #0a0a0a;
    }

    /* ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
    .stChatInput input {
        background-color: #111 !important;
        color: #FCEE0A !important;
        border: 2px solid #FCEE0A !important;
        border-radius: 0px !important;
    }
    
    /* ë¡œë”© ìŠ¤í”¼ë„ˆ ìƒ‰ìƒ */
    .stSpinner > div {
        border-top-color: #FCEE0A !important;
    }
    
    /* í•˜ë‹¨ Streamlit ë§ˆí¬ ìˆ¨ê¸°ê¸° (ì„ íƒì‚¬í•­) */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
</style>
""", unsafe_allow_html=True)

# í—¤ë” í‘œì‹œ
st.title("ğŸ”Œ NIGHT CITY ARCHIVES")
st.caption("ACCESSING SECURE DATASLATE... // WELCOME, EDGERUNNER.")

# --- 4. ë°ì´í„° ë¡œë“œ ë° RAG ì²´ì¸ êµ¬ì¶• (ìºì‹œ ì²˜ë¦¬) ---
@st.cache_resource
def load_database():
    try:
        loader = TextLoader(DATA_FILE, encoding="utf-8")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.split_documents(documents)

        embed_model = OpenAIEmbeddings(model="text-embedding-3-small")
        db = Chroma.from_documents(
            documents=docs, 
            embedding=embed_model, 
            persist_directory=CHROMA_DIR
        )
        retriever = db.as_retriever()
        
        llm = ChatOpenAI(model_name=RAG_MODEL)
        template = """
        ë‹¹ì‹ ì€ 'ì‚¬ì´ë²„í‘í¬ 2077' ì„¸ê³„ê´€ì˜ ì •í†µí•œ ì •ë³´ ë¸Œë¡œì»¤(Fixer)ì…ë‹ˆë‹¤.
        ë§íˆ¬ëŠ” ëƒ‰ì†Œì ì´ì§€ë§Œ ì •ë³´ëŠ” ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”. (ì˜ˆ: "~ë¼ê³  í•˜ë”êµ°.", "~ì•¼.")
        ì œê³µëœ ë°ì´í„°(Context)ì— ìˆëŠ” ë‚´ìš©ë§Œ ë‹µí•˜ê³ , ëª¨ë¥´ëŠ” ë‚´ìš©ì€ "ê·¸ê±´ ë‚´ ì •ë³´ë§ì— ì—†ëŠ” ë‚´ìš©ì´ì•¼."ë¼ê³  ë”± ì˜ë¼ ë§í•˜ì„¸ìš”.
        
        Context:
        {context}
        
        Question:
        {question}
        """
        prompt = ChatPromptTemplate.from_template(template)

        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        return rag_chain

    except Exception as e:
        st.error(f"âš ï¸ CRITICAL ERROR: DATABASE CORRUPTED. {e}")
        return None

rag_chain = load_database()

# --- 5. ì±„íŒ… UI ë° Multi-Turn êµ¬í˜„ ---
if rag_chain:
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": "ì›í•˜ëŠ” ì •ë³´ë¥¼ ë§í•´ë´. ê°€ê²©ì€... ë‚˜ì¤‘ì— ì²­êµ¬í•˜ì§€."})

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt_text := st.chat_input("ë°ì´í„°ë¥¼ ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì…ë ¥..."):
        st.session_state.messages.append({"role": "user", "content": prompt_text})
        with st.chat_message("user"):
            st.markdown(prompt_text)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            with st.spinner("ğŸ“¡ DECRYPTING SHARD DATA..."):
                full_response = rag_chain.invoke(prompt_text)
                message_placeholder.markdown(full_response)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})