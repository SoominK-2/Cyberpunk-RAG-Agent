import streamlit as st
import os
import sys
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. í™˜ê²½ ë³€ìˆ˜ ë° ì´ˆê¸° ì„¤ì • ---
# ğŸš¨ ì£¼ì˜: API í‚¤ë¥¼ ì—¬ê¸°ì— ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
# ì±—ë´‡ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•œ í‚¤ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
# ì‹¤ì œ ì œì¶œ ì‹œì—ëŠ” ì´ ë¶€ë¶„ì€ ì‚¬ìš©ìì—ê²Œ ë§¡ê¸°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# app.py íŒŒì¼ì˜ í‚¤ ì„¤ì • ë¶€ë¶„ì„ ì´ë ‡ê²Œ ë³€ê²½í•©ë‹ˆë‹¤.
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
RAG_MODEL = "gpt-4o-mini"
DATA_FILE = "cyberpunk_shards.txt"
CHROMA_DIR = "./cyberpunk_chroma_db"

# Streamlit ì•± ì œëª© ì„¤ì •
st.title("ì‚¬ì´ë²„í‘í¬ 2077 ì„¸ê³„ê´€ ë°±ê³¼ì‚¬ì „ AI")
st.caption("ì œê³µëœ ìƒ¤ë“œ ë°ì´í„°ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡ì…ë‹ˆë‹¤.")

# --- 2. ë°ì´í„° ë¡œë“œ ë° RAG ì²´ì¸ êµ¬ì¶• (ìºì‹œ ì²˜ë¦¬) ---

# @st.cache_resource: ì•±ì´ ì‹œì‘ë  ë•Œ ì´ í•¨ìˆ˜ë¥¼ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìºì‹œí•©ë‹ˆë‹¤.
@st.cache_resource
def load_database():
    try:
        # 1. í…ìŠ¤íŠ¸ ë¡œë“œ
        loader = TextLoader(DATA_FILE, encoding="utf-8")
        documents = loader.load()

        # 2. í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.split_documents(documents)

        # 3. ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ìƒì„±
        embed_model = OpenAIEmbeddings(model="text-embedding-3-small")
        db = Chroma.from_documents(
            documents=docs, 
            embedding=embed_model, 
            persist_directory=CHROMA_DIR
        )
        retriever = db.as_retriever()
        
        # 4. LLM ë° í”„ë¡¬í”„íŠ¸ ì •ì˜ (RAG_Chatbot.ipynbì˜ ì…€ 4 ë‚´ìš© ì¬ì‚¬ìš©)
        llm = ChatOpenAI(model_name=RAG_MODEL)
        template = """
        ë‹¹ì‹ ì€ 'ì‚¬ì´ë²„í‘í¬ 2077' ì„¸ê³„ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì œê³µëœ Context(ìƒ¤ë“œ ë‚´ìš©)ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ë§Œì•½ Contextì— ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê°€ ì•„ëŠ” ìƒ¤ë“œ ë‚´ìš© ì¤‘ì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
        
        Context:
        {context}
        
        Question:
        {question}
        """
        prompt = ChatPromptTemplate.from_template(template)

        # 5. RAG ì²´ì¸ êµ¬ì„± (RunnablePassthrough ì‚¬ìš©)
        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        return rag_chain

    except Exception as e:
        # íŒŒì¼ì´ ì—†ê±°ë‚˜ API í‚¤ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        st.error(f"ë°ì´í„° ë¡œë“œ ë˜ëŠ” DB êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.caption(f"'{DATA_FILE}' íŒŒì¼ê³¼ OpenAI API í‚¤ ì„¤ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        return None

# ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ë° RAG ì²´ì¸ ì´ˆê¸°í™”
rag_chain = load_database()

# --- 3. ì±„íŒ… UI ë° Multi-Turn êµ¬í˜„ ---

if rag_chain:
    # ì±— ê¸°ë¡ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì´ì „ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt_text := st.chat_input("ì‚¬ì´ë²„í‘í¬ ì„¸ê³„ê´€ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."):
        
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë¡ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt_text})
        with st.chat_message("user"):
            st.markdown(prompt_text)

        # 2. LLM í˜¸ì¶œ ë° ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("Night Cityì˜ ì§€ì‹ì„ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
                # RAG ì²´ì¸ í˜¸ì¶œ (Multi-turnì€ Streamlitì˜ messages historyë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.)
                # RAG ì²´ì¸ì´ ì§ˆë¬¸(prompt_text)ì„ ë°›ì•„ì„œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
                full_response = rag_chain.invoke(prompt_text)
                st.markdown(full_response)
        
        # 3. ë‹µë³€ì„ ê¸°ë¡
        st.session_state.messages.append({"role": "assistant", "content": full_response})